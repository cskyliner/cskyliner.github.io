---
title: 自然语言处理-统计语言模型与词表示
date: 2025-06-07
categories: [人工智能引论]
tags: [自然语言处理,词表示]
math: true
mermaid: true
toc: true
---

# 朴素贝叶斯模型

**贝叶斯公式**：

$$
P(B_i\mid A) = \frac{P(A\mid B_i)P(B_i)}{\sum_{j=1}^{n} P(B_j)P(A\mid B_j)}
$$

+ $P(B_i \mid \boldsymbol{A})$：后验概率（在观察到特征后样本属于类Bi的概率）
- $P(\boldsymbol{A} \mid B_i)$：似然（在给定类别Bi的条件下，观察到特征$\boldsymbol{A}$的概率）
+ $P(B_i)$：先验概率（样本属于类Bi的概率）
+ $P(\boldsymbol{A})$：证据（所有类别下观察到特征$\boldsymbol{A}$的总体概率）

由$A$的发生修改$B_i$ 发生的概率，执果求因。

**“朴素”**：假设总特征中的**所有特征**在**给定类别**的条件下**相互独立**，即：

$$P(\boldsymbol{A} \mid B_i) = \prod_{j=1}^{n} P(A_j \mid B_i)$$

**核心**(由相互独立与贝叶斯公式可以知道)：

$$P(B_i \mid \boldsymbol{A}) \propto P(B_i) \cdot \prod_{j=1}^{n} P(A_j \mid B_i)$$

以文本情感分类为例，朴素想法即为求出每个词在指定类别样本语句中发生的概率——比如正向情感样本中出现"love"的比例作为$P\left(\text{"love"}\mid postive\right)$——连乘然后归一化。

问题在于，一旦其中某个情况在样本中概率为零，整个预测将突变为零，造成失衡，因此需要使用某种手段进行“光滑”。

**加法光滑**：给统计得到的每种样本数量加上一个固定值$\alpha$

$$
P("loved"\mid postive) = \frac{包含"loved"的正面样本数量+\alpha}{正面样本总数量+词汇类别数 \times \alpha}
$$

**拉普拉斯光滑**：加法光滑的特例，令$\alpha = 1$ 

#  信息检索:tf-idf

**tf:词频**

$tf = {n \over N}$或 $tf = \log_{10}{(n + 1)}$

n： 某个词在文档中出现的次数 

N：文档中所有词出现的次数之和 

**idf:逆文档频率**

衡量某个词在语料库的所有文档中的罕见程度

$$
idf = \log_{10}(\frac{D}{1+d})
$$

$D$：语料库中包含的文档总数量 

$d$：语料库中出现某个词的文档数量

**tf-idf**:

$$
tfidf = tf \times idf
$$

用tf-idf值可以弱化常见词，保留重要的词。若某个词在某个文档中是高 频词，在整个语料中又是低频出现，那么这个词将具有高tf-idf值，它对 这篇文档来说，就是关键词，或主题词。

# 词袋模型(BoW)

1. 数据收集
2. 字典构建
3. 构建特征向量，字典中词的出现次数

缺点：字典可能极大；文本向量稀疏；关键词的重要性未体现

# 词表示

如何将词变为向量，将文本转化为数值特征进行机器学习：

## 独热表示

表示方法：

- 向量的维度等于字典的大小 V
- 对于字典中的每一个词，只在其对应位置上取值为 1，其他位置为 0

问题1：字典若很大，则词向量很长很稀疏

问题2：仅将词符号化，不包含任何语义信息，没有考虑词间的相关性

## 分布式表示

理论基础：

上下文相似的词，其语义也相似。词的语义由其上下文决定，而不是人为标注，同时考虑了词间相关性，这是其优势所在。

核心思想：

+ 选择一种方式描述上下文 /中心词
+ 选择一种模型刻画中心词与其上下文之间的关系
+ 进而训练该模型，通过输入词的描述来完成预测任务（上下文 ↔ 词）

### word2vec（基于学习）

#### CBOW模型：重点

1. 当前词的上下文词语的one-hot编码输入到输入层(例如当前词的前后两个词) 
2. 这些词向量分别乘以同一个矩阵$\boldsymbol{W}$（周围词向量矩阵）后分别得到各自的1 × N向量
3. 将这些1 × N向量取平均为一个1 × N向量
4. 将这个1 × N向量乘矩阵$\boldsymbol{W^{'}}$(中心词向量矩阵)，变成一个1 × N向量
5. 做Softmax分类，与真实标签1 × N向量计算交叉熵损失
6. 每次前向传播之后反向传播误差，调整矩阵$\boldsymbol{W}$ 和$\boldsymbol{W^{'}}$的值
7. 最后学习到的模型就是通过输入上下文转化为one-hot 向量输入，最后输出预测向量进而预测中心词，即通过上下文推断中心词

#### Skip-Gram模型

与CBOW模型对偶，输入中心词预测上下文。略
